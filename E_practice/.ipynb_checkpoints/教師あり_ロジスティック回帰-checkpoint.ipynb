{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward:\n",
    "    - $Z = W^{T}X+b$\n",
    "    - $Y_{hat}= 1/(1-exp(-Z))$\n",
    "- gradient descent:\n",
    "    - gradient:\n",
    "        - $G_{W}:$\n",
    "        - $G_{b}:$\n",
    "      \n",
    "    - loss function\n",
    "        -  $Loss =- \\sum_{i=1}^{m} ylog(y_{hat}) + (1-y)log(1-y_{hat}) * (1/m) $\n",
    "    - update\n",
    "        - $W \\leftarrow  W- \\alpha G_{W}$\n",
    "        - $b \\leftarrow b- \\alpha G_{b}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameter(n_features):\n",
    "    w = np.random.rand(n_features)\n",
    "    b =  np.random.rand()\n",
    "    return w,b\n",
    "\n",
    "def forward(X,w,b):\n",
    "    z = np.dot(X,w.T)+b\n",
    "    y = 1/(1+np.exp(-z))\n",
    "    return y\n",
    "\n",
    "def predict(X,w,b):\n",
    "    y_test_hat = forward(X,w,b)\n",
    "    y_test_hat = np.where(y_test_hat > 0.5, 1,0)\n",
    "    return y_test_hat\n",
    "\n",
    "def backward(y_hat , y):\n",
    "    m = y_hat.shape[0]\n",
    "    \n",
    "    dz = y_hat-y\n",
    "    dw = (1/m) *np.dot(dz.T,X)\n",
    "    db = (1/m) * np.sum(dz)\n",
    "    \n",
    "    grads = {\n",
    "        'dw':dw\n",
    "        ,'db':db\n",
    "    }\n",
    "    return grads \n",
    "    \n",
    "def loss(y_hat,y):\n",
    "    m = y.shape[0]\n",
    "    loss = -np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) / m \n",
    "    return loss\n",
    "\n",
    "def update(w,b,grads):\n",
    "    w = w - LR * grads['dw']  \n",
    "    b = b - LR * grads['db']\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 4)\n",
      "(70,)\n",
      "(30, 4)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "### initialize parameter\n",
    "ITER = 5\n",
    "LR = 0.01\n",
    "TEST_SIZE = 0.3\n",
    "grads = {\n",
    "    'dw':0\n",
    "    ,'db':0\n",
    "}\n",
    "\n",
    "### load data\n",
    "data_iris = load_iris()\n",
    "X,y = data_iris['data'],data_iris['target']\n",
    "idx = y!=2\n",
    "X,y =X[idx],y[idx]\n",
    "X,X_test ,y, y_test = train_test_split(X,y ,test_size = TEST_SIZE ,random_state = 0)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 , cost:3.346126985243548 ,accuracy:0.5 \n",
      "iter:10 , cost:2.335128163540932 ,accuracy:0.5 \n",
      "iter:20 , cost:1.3827842520886273 ,accuracy:0.5 \n",
      "iter:30 , cost:0.7081659842575997 ,accuracy:0.5 \n",
      "iter:40 , cost:0.47732044963172343 ,accuracy:0.7 \n",
      "iter:50 , cost:0.42589640757891134 ,accuracy:1.0 \n",
      "iter:60 , cost:0.40239853323440317 ,accuracy:1.0 \n",
      "iter:70 , cost:0.38325175124068966 ,accuracy:1.0 \n",
      "iter:80 , cost:0.36585167933153845 ,accuracy:1.0 \n",
      "iter:90 , cost:0.3498103879205458 ,accuracy:1.0 \n"
     ]
    }
   ],
   "source": [
    "#initialize\n",
    "w,b = initialize_parameter(X.shape[1])\n",
    "\n",
    "for i in range(100):\n",
    "#     forward\n",
    "    y_hat = forward(X,w,b)\n",
    "#     calculate grad,cost\n",
    "    grads = backward(y_hat , y)\n",
    "#     update\n",
    "    w,b = update(w,b,grads)\n",
    "\n",
    "    if i%10 == 0 :\n",
    "        cost = loss(y_hat , y)\n",
    "        y_test_hat = predict(X_test,w,b)\n",
    "        accuracy =  accuracy_score(y_test_hat,y_test)\n",
    "        print('iter:{} , cost:{} ,accuracy:{} '.format(i,cost,accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (3, 4)\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([[1,2,3],[4,5,6]])\n",
    "a2 = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "print(a1.shape,a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 38,  44,  50,  56],\n",
       "       [ 83,  98, 113, 128]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a1,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
